{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f84c1d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torchvision \n",
    "import numpy as np \n",
    "import PIL\n",
    "from PIL import Image, ImageFilter \n",
    "import os \n",
    "import random\n",
    "import glob\n",
    "import pdb\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.transforms.functional as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e1a4734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std(images_list) : \n",
    "    pixels = [] \n",
    "    for i, filepath in enumerate(images_list) : \n",
    "        img = Image.open(filepath)\n",
    "        try : \n",
    "            img = tf.to_tensor(img) \n",
    "            pixels.append(img.view(-1))\n",
    "        except TypeError : \n",
    "            print(f'{filepath} is truncated')\n",
    "        if i % 500 == 0 : \n",
    "            print(f'{i}/{len(images_list)}')\n",
    "        if i == 2000 :\n",
    "            break # Out of memory..\n",
    "    pixels = torch.cat(pixels, dim=0)\n",
    "    return torch.std_mean(pixels, dim=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d79cb1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sync_transform(*images) : \n",
    "    w,h = images[0].size # assuming w=512, h<=1024\n",
    "    assert w == 512 \n",
    "    if h < 1024 : \n",
    "        # pad to 1024\n",
    "        diff = 1024-h \n",
    "        images =  [tf.pad(image, (0,0,diff//2, diff-diff//2)) for image in images]\n",
    "    w,h = images[0].size\n",
    "    assert h == 1024\n",
    "    \n",
    "    # random horizontal flip.un\n",
    "        images = [tf.hflip(image) for image in images]\n",
    "    \n",
    "    # random pad\n",
    "    #if random.random() < 0.5 : \n",
    "    #    images = [tf.pad(image, 64) for image in images]\n",
    "    \n",
    "    # random rotation\n",
    "    angle = 0 \n",
    "    if random.random() < 0.5 : \n",
    "        angle = random.randint(-15, 15)\n",
    "        #images = [tf.rotate(image, angle, resample=PIL.Image.BILINEAR) for image in images]\n",
    "    \n",
    "    # random scale \n",
    "    scale = 1 \n",
    "    if random.random() < 0.5 : \n",
    "        scale = random.uniform(7/8, 9/8)\n",
    "    \n",
    "    images = [tf.affine(image, angle=angle, scale=scale, translate = (0,0), shear=0,\n",
    "                        resample = PIL.Image.BILINEAR) for image in images]\n",
    "    \n",
    "    images = [tf.pad(image, 64) for image in images]\n",
    "    \n",
    "    W,H = images[0].size \n",
    "    assert H >= h \n",
    "    assert W >= w \n",
    "    \n",
    "    h_diff = H-h\n",
    "    w_diff = W-w \n",
    "    \n",
    "    if random.random() < 0.5 : \n",
    "        # Center crop with 50% chance \n",
    "        h_start, w_start = h_diff//2, w_diff//2\n",
    "    else : \n",
    "        h_start, w_start = random.randint(0, h_diff), random.randint(0, w_diff)\n",
    "    \n",
    "    images = [tf.crop(image, h_start, w_start, h, w) for image in images]\n",
    "    \n",
    "    return images\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "246689da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integer_to_channels(target) : \n",
    "    target = (tf.to_tensor(target)*255).int()\n",
    "    c,h,w = target.shape \n",
    "    assert c == 1 \n",
    "    target = target.squeeze(0).numpy() # (h,w)\n",
    "    \n",
    "    output = np.zeros((h,w,3), dtype=np.uint8)\n",
    "    output[:,:,0] = (target==1).astype(np.uint8)*255\n",
    "    output[:,:,1] = (target==2).astype(np.uint8)*255\n",
    "    output[:,:,2] = (target==3).astype(np.uint8)*255\n",
    "    \n",
    "    return Image.fromarray(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e426948",
   "metadata": {},
   "outputs": [],
   "source": [
    "class oct_dataset(object) : \n",
    "    def __init__(self, data_path='./oct_data/images', label_path='./oct_data/labels', \n",
    "                 sync_transform = None, transform=None) : \n",
    "        self.data = []\n",
    "        self.transform=transform\n",
    "        self.sync_transform = sync_transform\n",
    "        self.totensor = torchvision.transforms.ToTensor()\n",
    "        \n",
    "        filenames = os.listdir(data_path)\n",
    "        self.data = [(os.path.join(data_path,filename),os.path.join(label_path,filename)) for filename in filenames]\n",
    "        \n",
    "            \n",
    "    def __len__(self) : \n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index) : \n",
    "        image_path, label_path = self.data[index]\n",
    "        image_name = os.path.basename(image_path)\n",
    "        \n",
    "        \n",
    "        image = Image.open(image_path)\n",
    "        label = Image.open(label_path)\n",
    "        label = integer_to_channels(label)\n",
    "        \n",
    "        if self.sync_transform is not None : \n",
    "            image, label = self.sync_transform(image, label)\n",
    "        if self.transform is not None : \n",
    "            image = self.transform(image)\n",
    "            \n",
    "        image = image.filter(ImageFilter.MedianFilter(size = 5)) \n",
    "        \n",
    "        image = self.totensor(image)\n",
    "        label = self.totensor(label)\n",
    "        \n",
    "        if True : \n",
    "            image = tf.normalize(image, 0.1410, 0.0941, inplace=True)\n",
    "            \n",
    "        label = label*255 \n",
    "        \n",
    "        return image, label,image_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb8e3df9",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] 지정된 경로를 찾을 수 없습니다: './oct_data/images'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19180/3980875817.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moct_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msync_transform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msync_transform\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msave_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'test_i.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msave_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'test_t.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19180/3960463922.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data_path, label_path, sync_transform, transform)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mfilenames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] 지정된 경로를 찾을 수 없습니다: './oct_data/images'"
     ]
    }
   ],
   "source": [
    "dataset = oct_dataset(sync_transform=sync_transform)\n",
    "image, target, name = dataset[1000]\n",
    "print(name)\n",
    "save_image(image, 'test_i.jpg')\n",
    "save_image(target, 'test_t.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37195d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#std, mean = get_mean_std(glob.glob('./oct_data/images/*.png'))\n",
    "#print(std, mean) # 0.0941, 0.1410 for 2000 samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdc1a3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img = Image.open('./oct_data/labels/190.png')\n",
    "tensor = tf.to_tensor(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9040bd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from : https://github.com/milesial/Pytorch-UNet\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "    \n",
    "    \n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512, 1024 // factor)\n",
    "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
    "        self.up2 = Up(512, 256 // factor, bilinear)\n",
    "        self.up3 = Up(256, 128 // factor, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf4e4751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1024, 512])\n"
     ]
    }
   ],
   "source": [
    "model = UNet(1, 3)\n",
    "criterion = torch.nn.BCELoss()\n",
    "output = model(dataset[0][0].unsqueeze(0))\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=2)\n",
    "print(output.shape)\n",
    "del output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2b5d72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6245, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\human\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "for i, (image, target, name) in enumerate(dataloader) : \n",
    "    output = model(image)\n",
    "    loss = criterion(F.sigmoid(output), target)\n",
    "    print(loss)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aece85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_.save('test.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769012cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(10)*10\n",
    "print(x, torch.round(x).int())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
